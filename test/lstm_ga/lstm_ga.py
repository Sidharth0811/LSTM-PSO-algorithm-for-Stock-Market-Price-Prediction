# -- coding: utf-8 --
"""LSTM_GA-ver1.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qCVs_D2kMHPocYLY0zjmZnJpWBfIyJha
"""

import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import matplotlib.pyplot as plt
import os


def stock_predict_GA(stock_name,filename):

    # Download stock data
    stock_data = yf.download(stock_name, start='2020-01-01', end='2024-01-01')
    specific_df = pd.DataFrame(stock_data).reset_index()

    # Use closing price as the target variable
    target_col = 'Close'

    # Feature engineering
    specific_df['Date'] = pd.to_datetime(specific_df['Date'])
    specific_df['Year'] = specific_df['Date'].dt.year
    specific_df['Month'] = specific_df['Date'].dt.month
    specific_df['Day'] = specific_df['Date'].dt.day

    # Normalize data
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(specific_df[[target_col]])

    # Define genetic algorithm parameters
    population_size = 20
    num_generations = 10
    mutation_rate = 0.02

    # Split data into training and testing sets
    train_size = int(len(scaled_data) * 0.8)
    train_data, test_data = scaled_data[0:train_size, :], scaled_data[train_size:len(scaled_data), :]

    # Function to generate sequences and labels for LSTM
    def generate_sequences_and_labels(data, n_past):
        sequences, labels = [], []
        for i in range(len(data)-n_past):
            sequences.append(data[i:i+n_past])
            labels.append(data[i+n_past, 0])
        return np.array(sequences), np.array(labels)

    # Set the number of past days to consider for prediction
    n_past = 60
    x_train, y_train = generate_sequences_and_labels(train_data, n_past)
    x_test, y_test = generate_sequences_and_labels(test_data, n_past)

    ################################### START ##########################################

    # Genetic Algorithm
    def initialize_population():
        return {
            'lstm_units': np.random.randint(50, 200),
            'dropout_rate': np.random.uniform(0.2, 0.5),
            'learning_rate': 10 ** np.random.uniform(-4, -2)
        }

    def create_lstm_model(lstm_units, dropout_rate, learning_rate):
        model = Sequential()
        model.add(LSTM(units=lstm_units, input_shape=(n_past, 1)))
        model.add(Dropout(dropout_rate))
        model.add(Dense(1))
        model.compile(optimizer='adam', loss='mean_squared_error')
        return model

    def fitness(params, x_train, y_train, x_test, y_test):
        lstm_units = params['lstm_units']
        dropout_rate = params['dropout_rate']
        learning_rate = params['learning_rate']

        model = create_lstm_model(lstm_units, dropout_rate, learning_rate)

        model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)

        train_predictions = model.predict(x_train)
        test_predictions = model.predict(x_test)

        train_mse = mean_squared_error(y_train, train_predictions)
        test_mse = mean_squared_error(y_test, test_predictions)

        # Maximize negative mean squared error
        return -test_mse

    def mutate(params):
        for key in params.keys():
            if np.random.rand() < mutation_rate:
                if key == 'lstm_units':
                    params[key] = np.random.randint(50, 200)
                elif key == 'dropout_rate':
                    params[key] = np.random.uniform(0.2, 0.5)
                elif key == 'learning_rate':
                    params[key] = 10 ** np.random.uniform(-4, -2)
        return params

    def crossover(parent1, parent2):
        child = {}
        for key in parent1.keys():
            child[key] = np.random.choice([parent1[key], parent2[key]])
        return child

    # Genetic Algorithm
    population = [initialize_population() for _ in range(population_size)]

    for generation in range(num_generations):
        fitness_scores = [fitness(params, x_train, y_train, x_test, y_test) for params in population]
        selected_indices = np.argsort(fitness_scores)[-population_size // 2:]
        selected_population = [population[i] for i in selected_indices]

        mutated_population = [mutate(params) for params in selected_population]
        crossover_indices = np.random.permutation(len(selected_population))
        offspring_population = [crossover(mutated_population[i], mutated_population[crossover_indices[i]]) for i in range(population_size // 2)]

        population = selected_population + offspring_population

    best_params = population[np.argmax([fitness(params, x_train, y_train, x_test, y_test) for params in population])]

    # Train the final LSTM model with the best hyperparameters
    best_lstm_model = create_lstm_model(best_params['lstm_units'], best_params['dropout_rate'], best_params['learning_rate'])
    best_lstm_model.fit(x_train, y_train, epochs=75, batch_size=32, verbose=2)

    ######################################## END #######################################

    folder_path = os.path.dirname(os.path.abspath(__file__))
    filename_a = os.path.join(folder_path, filename + "GA.keras")
    best_lstm_model.save(filename_a)

    #best_lstm_model = tf.keras.models.load_model(filename_a)

    # Make predictions
    def make_predictions(best_lstm_model, x_train, x_test):
        train_predict = best_lstm_model.predict(x_train)
        test_predict = best_lstm_model.predict(x_test)
        return train_predict, test_predict

    train_predict, test_predict = make_predictions(best_lstm_model, x_train, x_test)

    def inverse_transform(scaler, y_train, train_predict, y_test, test_predict):
        y_test = y_test.reshape(-1, 1)
        y_train = y_train.reshape(-1, 1)
        y_test = scaler.inverse_transform(y_test)
        y_train = scaler.inverse_transform(y_train)
        train_predict = scaler.inverse_transform(train_predict)
        test_predict = scaler.inverse_transform(test_predict)
        return y_test, y_train, train_predict, test_predict

    y_test, y_train, train_predict, test_predict = inverse_transform(scaler, y_train, train_predict, y_test, test_predict)

    def evaluate_model(y_train, train_predict, y_test, test_predict):
        # train_mse = mean_squared_error(y_train, train_predict)
        test_mse = mean_squared_error(y_test, test_predict)
        test_mae = mean_absolute_error(y_test, test_predict)
        test_mape = mean_absolute_percentage_error(y_test, test_predict)
        test_rs = r2_score(y_test, test_predict)
        # print(f"Training MSE: {train_mse}")
        print(f"MSE of {stock_name}: {test_mse}")
        print(f"MAE of {stock_name}: {test_mae}")
        print(f"MAPE of {stock_name}: {test_mape}")
        print(f"R Squared of {stock_name}: {test_rs}")

    evaluate_model(y_train, train_predict, y_test, test_predict)

    predictions = best_lstm_model.predict(x_test)
    actual_values_inverse = y_test

    predicted_values_inverse = scaler.inverse_transform(predictions)

    # Plot actual vs. predicted values
    plt.figure(figsize=(15, 6))
    plt.plot(actual_values_inverse.flatten(), label="Actual Close Prices")
    plt.plot(predicted_values_inverse.flatten(), 'r', label="Predicted Close Prices")
    plt.title(stock_name+'-LSTM GA')
    plt.ylabel('Close Price')
    plt.xlabel('Time Step')
    plt.legend()
    # plt.show()
    plt_name = os.path.join(folder_path, f"{stock_name}GA.png")
    plt.savefig(plt_name)
